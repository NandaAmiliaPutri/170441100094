{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Biodata Penulis Nama : Nanda Amilia Putri N.I.M : 170441100094 Program Studi : Sistem Informasi Jurusan : Teknik Informatika Universitas : Univ. Trunojoyo Madura","title":"Biografi"},{"location":"#biodata-penulis","text":"Nama : Nanda Amilia Putri N.I.M : 170441100094 Program Studi : Sistem Informasi Jurusan : Teknik Informatika Universitas : Univ. Trunojoyo Madura","title":"Biodata Penulis"},{"location":"KNN/","text":"Pengertian dan Cara Kerja Algoritma K-Nearest Neighbors (KNN) K-nearest neighbors atau KNN adalah algoritma yang berfungsi untuk melakukan klasifikasi suatu data berdasarkan data pembelajaran ( train data sets ), yang diambil dari k tetangga terdekatnya ( nearest neighbors ). Dengan k merupakan banyaknya tetangga terdekat. Persamaan dibawah ini menunjukkan rumus perhitungan untuk mencari jarak terdekat dengan d adalah jarak dan p adalah dimensi data(Agusta, 2007): Dengan keterangan : \ud835\udc651 : sampel data \ud835\udc652 : data uji i : data ke-i d : jarak euclidean p : dimensi data A. Cara Kerja Algoritma K-Nearest Neighbors (KNN) K-nearest neighbors melakukan klasifikasi dengan proyeksi data pembelajaran pada ruang berdimensi banyak. Ruang ini dibagi menjadi bagian-bagian yang merepresentasikan kriteria data pembelajaran. Setiap data pembelajaran direpresentasikan menjadi titik-titik c pada ruang dimensi banyak. Klasifikasi Terdekat (Nearest Neighbor Classification) Data baru yang diklasifikasi selanjutnya diproyeksikan pada ruang dimensi banyak yang telah memuat titik-titik c data pembelajaran. Proses klasifikasi dilakukan dengan mencari titik c terdekat dari c-baru ( nearest neighbor ) . Teknik pencarian tetangga terdekat yang umum dilakukan dengan menggunakan formula jarak euclidean . Berikut beberapa formula yang digunakan dalam algoritma knn. Banyaknya k Tetangga Terdekat Untuk menggunakan algoritma k nearest neighbors, perlu ditentukan banyaknya k tetangga terdekat yang digunakan untuk melakukan klasifikasi data baru. Banyaknya k, sebaiknya merupakan angka ganjil, misalnya k = 1, 2, 3, dan seterusnya. Penentuan nilai k dipertimbangkan berdasarkan banyaknya data yang ada dan ukuran dimensi yang dibentuk oleh data. Semakin banyak data yang ada, angka k yang dipilih sebaiknya semakin rendah. Namun, semakin besar ukuran dimensi data, angka k yang dipilih sebaiknya semakin tinggi. Algoritma K-Nearest Neighbors Tentukan k bilangan bulat positif berdasarkan ketersediaan data pembelajaran. Pilih tetangga terdekat dari data baru sebanyak k. Tentukan klasifikasi paling umum pada langkah (ii), dengan menggunakan frekuensi terbanyak. Keluaran klasifikasi dari data sampel baru. Implementasi KNN menggunakan data Iris dengan program bahasa Python import numpy as np from sklearn import neighbors, datasets from sklearn import preprocessing #author : Nanda AP n_neighbors = 20 # import module datasets iris iris = datasets.load_iris() # menyiapkan data X = iris.data[:, :150] y = iris.target # membuat instance dari Tetangga Klasifikasi dan menyesuaikan data. clf = neighbors.KNeighborsClassifier(n_neighbors, weights='distance') clf.fit(X, y) # membuat prediksi sl = input('Enter sepal length (cm): ') sw = input('Enter sepal width (cm): ') pl = input('Enter petal length (cm): ') pw = input('Enter petal width (cm): ') dataClass = clf.predict([[sl,sw,pl,pw]]) print('Prediction: '), if dataClass == 0: print('Iris Setosa') elif dataClass == 1: print('Iris Versicolour') else: print('Iris Virginica') Refrensi: Gorunescu, F. 2011. Data Mining Concept Model and Techniques. Berlin: Springer. ISBN 978-3-642-19720-8. Han, Jiawei dan Kamber, Micheline. (2006), Data Mining : Concept and Techniques Second Edition, Morgan Kaufmann Publishers. Florin Gorunescu, Data Mining: Concepts, Models and Techniques, Springer, 2011. https://cahyadsn.phpindonesia.id/extra/knn.php https://www.academia.edu/31306621/MAKALAH_KNN_K-NEAREST_NEIGHBOUR_","title":"K-Nearest Neighbors"},{"location":"KNN/#pengertian-dan-cara-kerja-algoritma-k-nearest-neighbors-knn","text":"K-nearest neighbors atau KNN adalah algoritma yang berfungsi untuk melakukan klasifikasi suatu data berdasarkan data pembelajaran ( train data sets ), yang diambil dari k tetangga terdekatnya ( nearest neighbors ). Dengan k merupakan banyaknya tetangga terdekat. Persamaan dibawah ini menunjukkan rumus perhitungan untuk mencari jarak terdekat dengan d adalah jarak dan p adalah dimensi data(Agusta, 2007): Dengan keterangan : \ud835\udc651 : sampel data \ud835\udc652 : data uji i : data ke-i d : jarak euclidean p : dimensi data","title":"Pengertian dan Cara Kerja Algoritma          K-Nearest Neighbors (KNN)"},{"location":"KNN/#a-cara-kerja-algoritma-k-nearest-neighbors-knn","text":"K-nearest neighbors melakukan klasifikasi dengan proyeksi data pembelajaran pada ruang berdimensi banyak. Ruang ini dibagi menjadi bagian-bagian yang merepresentasikan kriteria data pembelajaran. Setiap data pembelajaran direpresentasikan menjadi titik-titik c pada ruang dimensi banyak. Klasifikasi Terdekat (Nearest Neighbor Classification) Data baru yang diklasifikasi selanjutnya diproyeksikan pada ruang dimensi banyak yang telah memuat titik-titik c data pembelajaran. Proses klasifikasi dilakukan dengan mencari titik c terdekat dari c-baru ( nearest neighbor ) . Teknik pencarian tetangga terdekat yang umum dilakukan dengan menggunakan formula jarak euclidean . Berikut beberapa formula yang digunakan dalam algoritma knn. Banyaknya k Tetangga Terdekat Untuk menggunakan algoritma k nearest neighbors, perlu ditentukan banyaknya k tetangga terdekat yang digunakan untuk melakukan klasifikasi data baru. Banyaknya k, sebaiknya merupakan angka ganjil, misalnya k = 1, 2, 3, dan seterusnya. Penentuan nilai k dipertimbangkan berdasarkan banyaknya data yang ada dan ukuran dimensi yang dibentuk oleh data. Semakin banyak data yang ada, angka k yang dipilih sebaiknya semakin rendah. Namun, semakin besar ukuran dimensi data, angka k yang dipilih sebaiknya semakin tinggi.","title":"A. Cara Kerja Algoritma K-Nearest Neighbors (KNN)"},{"location":"KNN/#algoritma-k-nearest-neighbors","text":"Tentukan k bilangan bulat positif berdasarkan ketersediaan data pembelajaran. Pilih tetangga terdekat dari data baru sebanyak k. Tentukan klasifikasi paling umum pada langkah (ii), dengan menggunakan frekuensi terbanyak. Keluaran klasifikasi dari data sampel baru.","title":"Algoritma K-Nearest Neighbors"},{"location":"KNN/#implementasi-knn-menggunakan-data-iris-dengan-program-bahasa-python","text":"import numpy as np from sklearn import neighbors, datasets from sklearn import preprocessing #author : Nanda AP n_neighbors = 20 # import module datasets iris iris = datasets.load_iris() # menyiapkan data X = iris.data[:, :150] y = iris.target # membuat instance dari Tetangga Klasifikasi dan menyesuaikan data. clf = neighbors.KNeighborsClassifier(n_neighbors, weights='distance') clf.fit(X, y) # membuat prediksi sl = input('Enter sepal length (cm): ') sw = input('Enter sepal width (cm): ') pl = input('Enter petal length (cm): ') pw = input('Enter petal width (cm): ') dataClass = clf.predict([[sl,sw,pl,pw]]) print('Prediction: '), if dataClass == 0: print('Iris Setosa') elif dataClass == 1: print('Iris Versicolour') else: print('Iris Virginica')","title":"Implementasi KNN menggunakan data Iris dengan program bahasa Python"},{"location":"KNN/#refrensi","text":"Gorunescu, F. 2011. Data Mining Concept Model and Techniques. Berlin: Springer. ISBN 978-3-642-19720-8. Han, Jiawei dan Kamber, Micheline. (2006), Data Mining : Concept and Techniques Second Edition, Morgan Kaufmann Publishers. Florin Gorunescu, Data Mining: Concepts, Models and Techniques, Springer, 2011. https://cahyadsn.phpindonesia.id/extra/knn.php https://www.academia.edu/31306621/MAKALAH_KNN_K-NEAREST_NEIGHBOUR_","title":"Refrensi:"},{"location":"Kmeans/","text":"Data Mining menggunakan Clustering dengan Algoritma K-Means Clustering Clustering atau klasterisasi adalah metode pengelompokan data. Menurut Tan, 2006 clustering adalah sebuah proses untuk mengelompokkan data ke dalam beberapa cluster atau kelompok sehingga data dalam satu cluster memiliki tingkat kemiripan yang maksimum dan data antar cluster memiliki kemiripan yang minimum. Clustering merupakan metode segmentasi data yang sangat berguna dalam prediksi dan analisa masalah bisnis tertentu. Misalnya segmentasi pasar, marketing dan pemetaan zonasi wilayah. Algoritma K-Means K-Means merupakan salah satu metode clustering non hirarki yang berusaha mempartisi data yang ada ke dalam bentuk satu atau lebih cluster. Dalam metode K-Means data-data yang memiliki karakteristik yang sama diklaster dalam satu kelompok dan data yang memiliki karakteristik yang berbeda dikelompokkan dengan kelompok lain yang sesuai dengan karakteristik tersebut. Teori Manual Rumus perhitungan jarak menggunakan Rumus Euclidean Distance keterangan : d(x,y) = jarak objek antara objek x dan y n = dimensi data xi = nilai kolom yi = Tahapan Algoritma K-Means Langkah 1 \u2013 Menentukan secara acak K titik data sebagai pusat cluster yang disebut centroid. Langkah 2 \u2013 Menandai masing masing \ud835\udc65_\ud835\udc56 masuk ke ke cluster tertentu, dengan cara menghitung jarak \ud835\udc65_\ud835\udc56 ke masing masing pusat cluster (centroid) dan memasukkan \ud835\udc65_\ud835\udc56 anggota pusat cluster tertentu tersebut jika memiliki jarak terdekat. Langkah 3 \u2013 Menentukan pusat cluster baru dengan menghitungya rata rata dari anggota cluster Langkah 4 \u2013 Ulangai langkah 2 dan 3 sampai tidak tidak ada dari anggota setiap cluster berubah tempat kelompoknya Implementasi Dimisalkan kita memiliki sampel data wine quality. Dalam tabel berikut ada 15 buah data yang akan kita kelompokkan menjadi 3 cluster. Kita sebut saja C1, C2 dan C3. Kita menghitungnya menggunakan Excel. Pertama kita tentukan centorid nya. disini centoid diambil dari data ke -3, data ke-10 dan data ke-14 Kemudian kita hitung Iterasi pertama dari ke 15 data tersebut dengan centroid awal yang sudah ditentukan menggunakan rumus Euclidean Distance sehingga menghasilkan data dibawah ini. Kemuadian kita tentukan data minimalnya, setelah itu kita kelompokkan data sesuai pada cluster nya menurut perhitungan angka yang terkecil. Setelah data yang telah dihitung sudah terklompokkan sesuai cluster nya, kita cari rata-rata nya. Kemudian kita lakukan Iterasi kedua menggunakan centroid baru dari rata-rata kelompok cluster yang sudah dihitung sebelumnya Setelah kita menghitung Iterasi kedua dan cluster nya tidak berubah, maka Iterasi dihentikan. Jadi jika perhitungan pada Iterasi pertama dan Iterasi kedua sama, berarti data yang telah dihitung sudah terkelompokkan pada cluster yang benar. Refrensi: Agusta, Y. 2007. K-means - Penerapan, Permasalahan dan Metode Terkait. Jurnal Sistem dan Informatika Vol. 3 (Februari 2007): 47-60. Dalam jurnal SNTIKI, vol (5), Hal 395-398, oleh Nengsih W, Febiyanto pada tahun 2012 dengan judul \u201cData Mining Analysis Pengelompokan Penerima Beasiswa Menggunakan Teknik Clustering K-Means. https://id.wikipedia.org/wiki/K-means https://www.codepolitan.com/5-library-python-untuk-data-science-59b774b6cad97","title":"K-Means"},{"location":"Kmeans/#data-mining-menggunakan-clustering-dengan-algoritma-k-means","text":"","title":"Data Mining menggunakan Clustering dengan Algoritma K-Means"},{"location":"Kmeans/#clustering","text":"Clustering atau klasterisasi adalah metode pengelompokan data. Menurut Tan, 2006 clustering adalah sebuah proses untuk mengelompokkan data ke dalam beberapa cluster atau kelompok sehingga data dalam satu cluster memiliki tingkat kemiripan yang maksimum dan data antar cluster memiliki kemiripan yang minimum. Clustering merupakan metode segmentasi data yang sangat berguna dalam prediksi dan analisa masalah bisnis tertentu. Misalnya segmentasi pasar, marketing dan pemetaan zonasi wilayah.","title":"Clustering"},{"location":"Kmeans/#algoritma-k-means","text":"K-Means merupakan salah satu metode clustering non hirarki yang berusaha mempartisi data yang ada ke dalam bentuk satu atau lebih cluster. Dalam metode K-Means data-data yang memiliki karakteristik yang sama diklaster dalam satu kelompok dan data yang memiliki karakteristik yang berbeda dikelompokkan dengan kelompok lain yang sesuai dengan karakteristik tersebut.","title":"Algoritma K-Means"},{"location":"Kmeans/#teori-manual","text":"Rumus perhitungan jarak menggunakan Rumus Euclidean Distance keterangan : d(x,y) = jarak objek antara objek x dan y n = dimensi data xi = nilai kolom yi =","title":"Teori Manual"},{"location":"Kmeans/#tahapan-algoritma-k-means","text":"Langkah 1 \u2013 Menentukan secara acak K titik data sebagai pusat cluster yang disebut centroid. Langkah 2 \u2013 Menandai masing masing \ud835\udc65_\ud835\udc56 masuk ke ke cluster tertentu, dengan cara menghitung jarak \ud835\udc65_\ud835\udc56 ke masing masing pusat cluster (centroid) dan memasukkan \ud835\udc65_\ud835\udc56 anggota pusat cluster tertentu tersebut jika memiliki jarak terdekat. Langkah 3 \u2013 Menentukan pusat cluster baru dengan menghitungya rata rata dari anggota cluster Langkah 4 \u2013 Ulangai langkah 2 dan 3 sampai tidak tidak ada dari anggota setiap cluster berubah tempat kelompoknya","title":"Tahapan Algoritma K-Means"},{"location":"Kmeans/#implementasi","text":"Dimisalkan kita memiliki sampel data wine quality. Dalam tabel berikut ada 15 buah data yang akan kita kelompokkan menjadi 3 cluster. Kita sebut saja C1, C2 dan C3. Kita menghitungnya menggunakan Excel. Pertama kita tentukan centorid nya. disini centoid diambil dari data ke -3, data ke-10 dan data ke-14 Kemudian kita hitung Iterasi pertama dari ke 15 data tersebut dengan centroid awal yang sudah ditentukan menggunakan rumus Euclidean Distance sehingga menghasilkan data dibawah ini. Kemuadian kita tentukan data minimalnya, setelah itu kita kelompokkan data sesuai pada cluster nya menurut perhitungan angka yang terkecil. Setelah data yang telah dihitung sudah terklompokkan sesuai cluster nya, kita cari rata-rata nya. Kemudian kita lakukan Iterasi kedua menggunakan centroid baru dari rata-rata kelompok cluster yang sudah dihitung sebelumnya Setelah kita menghitung Iterasi kedua dan cluster nya tidak berubah, maka Iterasi dihentikan. Jadi jika perhitungan pada Iterasi pertama dan Iterasi kedua sama, berarti data yang telah dihitung sudah terkelompokkan pada cluster yang benar.","title":"Implementasi"},{"location":"Kmeans/#refrensi","text":"Agusta, Y. 2007. K-means - Penerapan, Permasalahan dan Metode Terkait. Jurnal Sistem dan Informatika Vol. 3 (Februari 2007): 47-60. Dalam jurnal SNTIKI, vol (5), Hal 395-398, oleh Nengsih W, Febiyanto pada tahun 2012 dengan judul \u201cData Mining Analysis Pengelompokan Penerima Beasiswa Menggunakan Teknik Clustering K-Means. https://id.wikipedia.org/wiki/K-means https://www.codepolitan.com/5-library-python-untuk-data-science-59b774b6cad97","title":"Refrensi:"},{"location":"Tree/","text":"Decision Tree Classifier (Gain) Pohon keputusan adalah salah satu metode klasifikasi yang paling populer karena mudah untuk diinterpretasi oleh manusia. Pohon keputusan adalah model prediksi menggunakan struktur pohon atau struktur berhirarki. Konsep dari pohon keputusan adalah mengubah data menjadi pohon keputusan dan aturan-aturan keputusan. Manfaat utama dari penggunaan pohon keputusan adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simpel sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. \u200b Pohon Keputusan juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Pohon keputusan memadukan antara eksplorasi data dan pemodelan, sehingga sangat bagus sebagai langkah awal dalam proses pemodelan bahkan ketika dijadikan sebagai model akhir dari beberapa teknik lain. Sering terjadi tawar menawar antara keakuratan model dengan transparansi model. Dalam beberapa aplikasi, akurasi dari sebuah klasifikasi atau prediksi adalah satu-satunya hal yang ditonjolkan, misalnya sebuah perusahaan direct mail membuat sebuah model yang akurat untuk memprediksi anggota mana yang berpotensi untuk merespon permintaan, tanpa memperhatikan bagaimana atau mengapa model tersebut bekerja. Kelebihan Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global, dapat diubah menjadi simple dan spesifik. Eliminasi perhitungan-perhitungan yang tidak diperlukan, karena ketika menggunakan metode pohon keputusan maka contoh diuji hanya berdasarkan kriteria atau kelas-kelas tertentu. Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama. Metode pohon keputusan dapat menghindari munculnya permasalahan ini dengan menggunakan kriteria yang jumlahnya lebih sedikit pada setiap node internal tanpa banyak mengurangi kualitas keputusan yang dihasilkan. Kekurangan Terjadi overlap terutama ketika kelas-kelas dan kriteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah pohon keputusan yang besar. Kesulitan dalam mendesain pohon keputusan yang optimal Hasil kualitas keputusan yang didapatkan dari metode pohon keputusan sangat tergantung pada bagaimana pohon tersebut didesain. Arsitektur Pohon Keputusan Arsitektur pohon keputusan dibuat menyerupai bentuk pohon, dimana pada umumnya sebuah pohon terdapat akar (root), cabang dan daun (leaf). Pada pohon keputusan juga terdiri dari tiga bagian sebagai berikut : a. Root node atau node akar merupakan node yang terletak paling atas dari suatu pohon. b. Internal Node ini merupakan node percabangan, dimana pada node ini hanya terdapat satu input dan mempunyai minimal dua output. c. Leaf Node ini merupakan node akhir, hanya memiliki satu input, dan tidak memiliki output. Pada pohon keputusan setiap leaf node menandai label kelas. Gambar berikut merupakan bentuk arsitektur pohon keputusan. Algoritma Pohon dibangun dalam suatu metoda rekursif topdown divide and-conquer. Seluruh contoh pelatihan dimulai dari simpul root, lalu dilakukan penujian. Mencabang ke jalur yang benar berdasarkan hasil pengujian. Apakah simpul leaf ditemukan? Jika true , masukkan ke kelas target, jika false kembali ke langkah awal. Atribut-atribut berada dalam suatu kategori (jika bernilai kontinu, nilai-nilai tersebut didistribusikan terlebih dahulu). Contoh-contoh dipartisi secara rekursif berdasarkan atribut terpilih. Atribut-atribut uji dipilih berdasarakn heuristik atau pengukurann statistik (misal, information gain ). Entropy & Information Gain Algoritma pada metode ini menggunakan konsep dari entropi. Konsep Entropi yang digunakan untuk mengukur \u201cseberapa informatifnya\u201d sebuah node (yang biasanya disebut seberapa baiknya). Entropi(S) = 0, jika semua contoh pada S berada dalam kelas yang sama. Entroiy(S) = 1, jika jumlah contoh positif dan jumlah contoh negatif dalam S adalah sama. 0 < Entropi(S) < 1, jika jumlah contoh positif dan negatif dalam S tidak sama. Dimana: \u2022 S adalah himpunan (dataset) kasus \u2022 k adalah banyaknya partisi S \u2022 pj adalah probabilitas yang di dapat dari Sum(Ya) dibagi Total Kasus. Setelah mendapat nilai entropi, pemilihan atribut dilakukan dengan nilai information gain terbesar. IMPLEMENTASI (studi kasus : JOB APPLICANT) Sebelum menerapkan konsep decision tree pada studi kasus yang telah ditentukan, beberapa tools yang perlu dipersiapkan agar program yang kita rancang bisa dieksekusi dengan baik diantaranya: python 3.x (versi 3 keatas). Anaconda Navigator Berikut source code dan penjelasan untuk menyelesaikan study kasus tersebut dengan konsep Decision Tree Clasification. Pertama Import beberapa library dari python seperti: pandas => memuat sebuah file ke dalam tabel virtual ala spreadsheet yang memiliki struktur data yang diperukan untuk membersihkan data mentah ke dalam sebuah bentuk yang cocok untuk dianalisis. numpy => untuk operasi vektor dan matriks. Fiturnya hampir sama dengan MATLAB dalam mengelola array dan array multidimensi. sklearn => untuk mengimportkan library data science . Berbagai fungsi didalamnya seperti fungsi agregasi, hitung metriks, hitung akurasi, display gambar, dan lain sebagainya. seaborn => library untuk membuat grafik statistik. pydotplus => library untuk memvisualisasikan bentuk hirarki. #import library import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn import metrics from sklearn.metrics import accuracy_score import seaborn as sns from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO from IPython.display import Image from sklearn.tree import export_graphviz import pydotplus import numpy as np Kedua Mengimport data dari komputer dengan perintah pandas. data=pd.read_csv('nilai.csv') #Pastikan file data set berada dalam folder yang sama dengan file jupyter notebook Ketiga Menampilkan data. data.head() Keempat Melihat info kolom dari data. data.info() Kelima Memilih kolom uji untuk dihitung hasilnya. zero_not_accepted = ['pelamar','ipk','umur','nilai'] # for col in zero_not_accepted: # for i in data[col]: # if i==0: # colSum = sum(data[col]) # meanCol=colSum/len(data[col]) # data[col]=meanCol for col in zero_not_accepted: data[col]= data[col].replace(0,np.NaN) mean = int(data[col].mean(skipna=True)) data[col] = data[col].replace(np.NaN,mean) Keenam Membagi data train dan data test dengan data test 30%. X = data.iloc[:,0:3] #memilih objek data X dengan array y = data.iloc[:,3] #memilih objek data y dengan array #build model & train data X = data[['pelamar','ipk','umur','nilai']] y = data['hasil'] #split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0) Ketujuh Menentukan entropy data. clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4) clf = clf.fit(X_train,y_train) y_pred = clf.predict(X_test) Kedelapan Meenentukan simpul root, simpul perantara, dan simpul leaf dari data yang telah diketahui nilai entropy-nya. feature_cols = ['pelamar','ipk','umur','nilai'] dot_data = StringIO() export_graphviz(clf, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=['TIDAK LULUS','LULUS']) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_png('job.png') Image(graph.create_png()) Referensi https://en.wikipedia.org/wiki/Decision_tree. http://tessy.lecturer.pens.ac.id/kuliah/db2/klasifikasi.pdf . Discovering Knowledge in Data (Introduction to Data Mining), Chapter 6, Daniel T. Larose, Wiley, 2004.","title":"Decision Tree"},{"location":"Tree/#decision-tree-classifier-gain","text":"Pohon keputusan adalah salah satu metode klasifikasi yang paling populer karena mudah untuk diinterpretasi oleh manusia. Pohon keputusan adalah model prediksi menggunakan struktur pohon atau struktur berhirarki. Konsep dari pohon keputusan adalah mengubah data menjadi pohon keputusan dan aturan-aturan keputusan. Manfaat utama dari penggunaan pohon keputusan adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simpel sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. \u200b Pohon Keputusan juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Pohon keputusan memadukan antara eksplorasi data dan pemodelan, sehingga sangat bagus sebagai langkah awal dalam proses pemodelan bahkan ketika dijadikan sebagai model akhir dari beberapa teknik lain. Sering terjadi tawar menawar antara keakuratan model dengan transparansi model. Dalam beberapa aplikasi, akurasi dari sebuah klasifikasi atau prediksi adalah satu-satunya hal yang ditonjolkan, misalnya sebuah perusahaan direct mail membuat sebuah model yang akurat untuk memprediksi anggota mana yang berpotensi untuk merespon permintaan, tanpa memperhatikan bagaimana atau mengapa model tersebut bekerja.","title":"Decision Tree Classifier (Gain)"},{"location":"Tree/#kelebihan","text":"Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global, dapat diubah menjadi simple dan spesifik. Eliminasi perhitungan-perhitungan yang tidak diperlukan, karena ketika menggunakan metode pohon keputusan maka contoh diuji hanya berdasarkan kriteria atau kelas-kelas tertentu. Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama. Metode pohon keputusan dapat menghindari munculnya permasalahan ini dengan menggunakan kriteria yang jumlahnya lebih sedikit pada setiap node internal tanpa banyak mengurangi kualitas keputusan yang dihasilkan.","title":"Kelebihan"},{"location":"Tree/#kekurangan","text":"Terjadi overlap terutama ketika kelas-kelas dan kriteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah pohon keputusan yang besar. Kesulitan dalam mendesain pohon keputusan yang optimal Hasil kualitas keputusan yang didapatkan dari metode pohon keputusan sangat tergantung pada bagaimana pohon tersebut didesain.","title":"Kekurangan"},{"location":"Tree/#arsitektur-pohon-keputusan","text":"Arsitektur pohon keputusan dibuat menyerupai bentuk pohon, dimana pada umumnya sebuah pohon terdapat akar (root), cabang dan daun (leaf). Pada pohon keputusan juga terdiri dari tiga bagian sebagai berikut : a. Root node atau node akar merupakan node yang terletak paling atas dari suatu pohon. b. Internal Node ini merupakan node percabangan, dimana pada node ini hanya terdapat satu input dan mempunyai minimal dua output. c. Leaf Node ini merupakan node akhir, hanya memiliki satu input, dan tidak memiliki output. Pada pohon keputusan setiap leaf node menandai label kelas. Gambar berikut merupakan bentuk arsitektur pohon keputusan.","title":"Arsitektur Pohon Keputusan"},{"location":"Tree/#algoritma","text":"Pohon dibangun dalam suatu metoda rekursif topdown divide and-conquer. Seluruh contoh pelatihan dimulai dari simpul root, lalu dilakukan penujian. Mencabang ke jalur yang benar berdasarkan hasil pengujian. Apakah simpul leaf ditemukan? Jika true , masukkan ke kelas target, jika false kembali ke langkah awal. Atribut-atribut berada dalam suatu kategori (jika bernilai kontinu, nilai-nilai tersebut didistribusikan terlebih dahulu). Contoh-contoh dipartisi secara rekursif berdasarkan atribut terpilih. Atribut-atribut uji dipilih berdasarakn heuristik atau pengukurann statistik (misal, information gain ).","title":"Algoritma"},{"location":"Tree/#entropy-information-gain","text":"Algoritma pada metode ini menggunakan konsep dari entropi. Konsep Entropi yang digunakan untuk mengukur \u201cseberapa informatifnya\u201d sebuah node (yang biasanya disebut seberapa baiknya). Entropi(S) = 0, jika semua contoh pada S berada dalam kelas yang sama. Entroiy(S) = 1, jika jumlah contoh positif dan jumlah contoh negatif dalam S adalah sama. 0 < Entropi(S) < 1, jika jumlah contoh positif dan negatif dalam S tidak sama. Dimana: \u2022 S adalah himpunan (dataset) kasus \u2022 k adalah banyaknya partisi S \u2022 pj adalah probabilitas yang di dapat dari Sum(Ya) dibagi Total Kasus. Setelah mendapat nilai entropi, pemilihan atribut dilakukan dengan nilai information gain terbesar.","title":"Entropy &amp; Information Gain"},{"location":"Tree/#implementasi-studi-kasus-job-applicant","text":"Sebelum menerapkan konsep decision tree pada studi kasus yang telah ditentukan, beberapa tools yang perlu dipersiapkan agar program yang kita rancang bisa dieksekusi dengan baik diantaranya: python 3.x (versi 3 keatas). Anaconda Navigator Berikut source code dan penjelasan untuk menyelesaikan study kasus tersebut dengan konsep Decision Tree Clasification.","title":"IMPLEMENTASI (studi kasus : JOB APPLICANT)"},{"location":"Tree/#pertama","text":"Import beberapa library dari python seperti: pandas => memuat sebuah file ke dalam tabel virtual ala spreadsheet yang memiliki struktur data yang diperukan untuk membersihkan data mentah ke dalam sebuah bentuk yang cocok untuk dianalisis. numpy => untuk operasi vektor dan matriks. Fiturnya hampir sama dengan MATLAB dalam mengelola array dan array multidimensi. sklearn => untuk mengimportkan library data science . Berbagai fungsi didalamnya seperti fungsi agregasi, hitung metriks, hitung akurasi, display gambar, dan lain sebagainya. seaborn => library untuk membuat grafik statistik. pydotplus => library untuk memvisualisasikan bentuk hirarki. #import library import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn import metrics from sklearn.metrics import accuracy_score import seaborn as sns from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO from IPython.display import Image from sklearn.tree import export_graphviz import pydotplus import numpy as np","title":"Pertama"},{"location":"Tree/#kedua","text":"Mengimport data dari komputer dengan perintah pandas. data=pd.read_csv('nilai.csv') #Pastikan file data set berada dalam folder yang sama dengan file jupyter notebook","title":"Kedua"},{"location":"Tree/#ketiga","text":"Menampilkan data. data.head()","title":"Ketiga"},{"location":"Tree/#keempat","text":"Melihat info kolom dari data. data.info()","title":"Keempat"},{"location":"Tree/#kelima","text":"Memilih kolom uji untuk dihitung hasilnya. zero_not_accepted = ['pelamar','ipk','umur','nilai'] # for col in zero_not_accepted: # for i in data[col]: # if i==0: # colSum = sum(data[col]) # meanCol=colSum/len(data[col]) # data[col]=meanCol for col in zero_not_accepted: data[col]= data[col].replace(0,np.NaN) mean = int(data[col].mean(skipna=True)) data[col] = data[col].replace(np.NaN,mean)","title":"Kelima"},{"location":"Tree/#keenam","text":"Membagi data train dan data test dengan data test 30%. X = data.iloc[:,0:3] #memilih objek data X dengan array y = data.iloc[:,3] #memilih objek data y dengan array #build model & train data X = data[['pelamar','ipk','umur','nilai']] y = data['hasil'] #split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)","title":"Keenam"},{"location":"Tree/#ketujuh","text":"Menentukan entropy data. clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4) clf = clf.fit(X_train,y_train) y_pred = clf.predict(X_test)","title":"Ketujuh"},{"location":"Tree/#kedelapan","text":"Meenentukan simpul root, simpul perantara, dan simpul leaf dari data yang telah diketahui nilai entropy-nya. feature_cols = ['pelamar','ipk','umur','nilai'] dot_data = StringIO() export_graphviz(clf, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=['TIDAK LULUS','LULUS']) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_png('job.png') Image(graph.create_png())","title":"Kedelapan"},{"location":"Tree/#referensi","text":"https://en.wikipedia.org/wiki/Decision_tree. http://tessy.lecturer.pens.ac.id/kuliah/db2/klasifikasi.pdf . Discovering Knowledge in Data (Introduction to Data Mining), Chapter 6, Daniel T. Larose, Wiley, 2004.","title":"Referensi"}]}